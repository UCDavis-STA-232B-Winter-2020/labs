---
title: "STA232Lab4"
author: "Po"
date: "1/27/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# HW 2.1

Consider the Neyman-Scott problem (Example 9).

(a) Show that the MLE of $\sigma^2$ based on
$y_{ij}$, $i = 1,...,m$,$j = 1,2$ converges in probability to $\sigma^2 /2$, and therefore is inconsistent.

(b) Show that the REML estimator of $\sigma^2$, that is, the MLE of $\sigma^2$ based on $z_i$, $i = 1, . . . , m$, converges in probability to $\sigma^2$, and therefore is consistent.

Read also Example 1.7 and Exercise 1.8 in textbook Jiang 2007.

Example 1.7 (Neymanâ€“Scott problem). Neyman and Scott (1948) gave the following example which shows that, when the number of parameters increases with the sample size, the MLE may not be consistent. Suppose that two observations are collected from m individuals. Each individual has its own (unknown) mean, say, $\mu_i$ for the $i$th individual. Suppose that the observations are independent and normally distributed with variance $\sigma^2$. The problem of interest is to estimate $\sigma^2$. The model may be expressed as the following, $y_{ij} = \mu_i + \epsilon_{ij}$, where $\epsilon_{ij}$ are independent and distributed as $N(0, \sigma^2)$. Note that this may be viewed as a special case of the linear mixed model (1.1), in which $Z = 0$. However, it can be shown that the MLE of $\sigma^2$ is inconsistent (Exercise 1.8).

#### Hints:

https://en.wikipedia.org/wiki/Consistent_estimator

https://en.wikipedia.org/wiki/Convergence_of_random_variables

https://en.wikipedia.org/wiki/Law_of_large_numbers

https://en.wikipedia.org/wiki/Markov%27s_inequality

# HW 2.2

Exercise 1.12 of Jiang (2007).

1.12. Show that, under the balanced one-way random effects model (i.e., Example 1.1 with $k_i = k$, $1 \leq i \leq m$), the REML equations for estimating $\sigma^2$ and $\tau^2$ are equivalent to (1.22). Obtain the solution to these equations. Also derive the asymptotic covariance matrix of the REML estimators.

\begin{align*}
y_{ij} &= \mu + \alpha_i + \epsilon_{ij} \\
i &= 1, \dots, m \\
j &= 1, \dots, k \\
\alpha_i &\sim N(0, \sigma^2) \\
\epsilon_{ij} &\sim N(0, \tau^2)
\end{align*}

#### Hints:

https://en.wikipedia.org/wiki/Kronecker_product

https://en.wikipedia.org/wiki/Woodbury_matrix_identity

$$
\begin{aligned}
Y &= ( y_{11}, \dots, y_{1k}, \dots, y_{m1}, \dots, y_{mk})' \\
X &= 1_m \otimes 1_k \\
Z &= I_m \otimes 1_k \\
\end{aligned}
$$
\newcommand{\by}{\mathbf{y}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\be}{\boldsymbol{\epsilon}}
\newcommand{\ba}{\boldsymbol{\alpha}}
\newcommand{\bb}{\boldsymbol{\beta}}

\newcommand{\bX}{\mathbf{X}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bOne}{\mathbf{1}}

\newcommand{\diag}{\text{diag}}
\newcommand{\Var}{\text{Var}}
\newcommand{\E}{\text{E}}


$$
\begin{aligned}
V &= I_m \otimes (\tau^2 I_k + \sigma^2 1_k 1_k') \\
V^{-1} &= I_m \otimes \frac{1}{\tau^2} \left( I_k - \frac{\sigma^2}{\tau^2 + k \sigma^2} 1_k 1_k' \right)
\end{aligned}
$$

$$\bP = \dfrac{1}{\tau^2}\diag(\bV_k,...,\bV_k) - \dfrac{1}{mk(\tau^2 + k\sigma^2)}\bOne_{mk} \bOne_{mk}'$$ where $\bV_k$ is a $k\times k$ matrix with  diagonals $1 - \dfrac{\sigma^2}{\tau^2 + k\sigma^2}$ and off-diagonals $\dfrac{-\sigma^2}{\tau^2 + k\sigma^2}$.

Then try you best to simplify the matrix equations into the equations required by the exercise.

For the estimated asymptotic covariance matrix of $\hat{\theta}$, it will involve the matrices $\bZ$ and $\bP$. You can further express each entry in estimated asymptotic covariance matrix in terms of things like $k$, $m$, MSA, MSE...

# HW 2.3

Exercise 1.14 in Jiang (2007).

1.14. Show that the REML equations derived under the multivariate $t$-distribution (see Section 1.4.1) are equivalent to those derived under the multivariate normal distribution.

#### Hints:

I can guide you to show the REML equation under the ANOVA model ( equation (1.2) ).

Firstly, as a warm-up, try deriving REML equations (1.23) and (1.25) in Gaussian distribution in ANOVA model. It is exercise 1.13.

Now get to the t-distribution business. You should be able to get:
$$l_R(\theta) = c - \frac{1}{2}\log(|A'VA|) - \frac{n-p+d}{2}\log\left(1 + \frac{1}{d}y'Py \right)$$
and then the REML equation is
$$
(n-p+d)y'P\frac{\partial V}{\partial \theta}Py = tr\left( P\frac{\partial V}{\partial \theta}\right) (d+y'Py)
$$

Here, if $d+y'Py = n-p+d$, you get the REML equation exactly as with Gaussian.

Then, assume $\bV$ is in the form of ANOVA model, i.e. $V = \tau^2I_n + \sum_{i=1}^s\sigma_i^2Z_iZ_i'$. Plug-in the $\tau$ and $\sigma_i$ into the above. Then try to get $y'Py = n-p$.

Here are some equations you should be able to show:

$p = \text{rank}(X)$

$\text{rank}(A) = n - p$

$\text{tr}(PV) = n - p$.

$PVP = P$

https://en.wikipedia.org/wiki/Trace_(linear_algebra)

https://en.wikipedia.org/wiki/Rank_(linear_algebra)

At the end, you can show the above t-distribution business results in equation (1.23) which was originally derived from Gaussain distribution.

# HW 2.4

For the Lambs data of Project 1, derive a Wald-test statistic, in as explicit form as possible, for testing the hypothesis that there is no difference among the lines. What is the asymptotic distribution of the Wald-test?

#### Hints:

Page 133 in lecture note.

Page 177 in lecture note.

You should be able to explicitly write down the matrices $\hat{V}$, $\hat{V}^{-1}$ in terms of $\tau$, $\sigma$ and $n_{ij}$. Then you explicitly write down $X' \hat{V}^{-1} X$ in terms of $\hat{\tau}$, $\hat{\sigma}$, $x_{ijk}$ and $n_{ij}$.

## Lamb data

Let's look at the Lamb dataset in the textbook (the table 1.2 in Jiang 2007, same as the table 1 in Harville 1985).

>    Harville, David A., and Alan P. Fenech. "Confidence Intervals for a Variance Ratio, or for Heritability, in an Unbalanced Mixed Linear Model." Biometrics 41, no. 1 (1985): 137-52. doi:10.2307/2530650. <br>
> https://www.jstor.org/stable/2530650

> For purposes of illustration, we introduce in Table 1 some data consisting of the weights at
> birth of 62 single-birth male lambs. These data (provided by G. E. Bradford, Department
> of Animal Science, University of California, Davis) come from five distinct population
> lines (two control lines and three selection lines). Each lamb was the progeny of one of 23
> rams, and each lamb had a different dam. Age of dam was recorded as belonging to one of
> three categories, numbered 1 (1-2 years), 2 (2-3 years), and 3 (over 3 years).

Sheep glossary: http://www.sheep101.info/201/glossary.html

`lamb_lab1.csv` contains a subset of the data (Line 1 and 2).

```{r message=FALSE}
library(lme4)

lamb <- read.csv(file = "lamb_lab1.csv")
cols <- c("Sire", "Line", "Age")
lamb[cols] <- lapply(lamb[cols], as.factor)

knitr::kable(lamb)
```

Let's fit a linear mixed effect model!
```{r}
lamb_fitted <- lmer(Weight ~ Line + Age - 1 + (1|Sire), data=lamb)
summary(lamb_fitted)
```

```{r}
lamb_fitted_mle <- lmer(Weight ~ Line + Age - 1 + (1|Sire), data=lamb, REML = FALSE)
summary(lamb_fitted_mle)
```

$\mathbf{X}$ and $\mathbf{Z}$
```{r}
happy_X <- getME(lamb_fitted, "X")
happy_Z <- getME(lamb_fitted, "Z")
```

$\mathbf{X} \hat{\boldsymbol{\beta}}$
```{r}
happy_Xbeta = happy_X %*% fixef(lamb_fitted)
```

EBLUP of $\mathbf{Z} \boldsymbol{\alpha}$
```{r}
happy_Zalpha = as.matrix(happy_Z %*% ranef(lamb_fitted)$Sire$`(Intercept)`)
```

```{r}
knitr::kable(cbind(happy_Xbeta, happy_Zalpha, happy_Xbeta+happy_Zalpha, predict(lamb_fitted), lamb))
```

Variance and Correlation Components
```{r}
VarCorr(lamb_fitted)
```

```{r}
sqrt(unlist(VarCorr(lamb_fitted)))
```

```{r}
sigma(lamb_fitted)
```

```{r}
?getME
?VarCorr
?fixef
?ranef
```

Simulation with fitted parameters.
```{r}
simulate(lamb_fitted, nsim = 5, seed = 1999)
```

Lets do parametric bootstrap!
```{r}
mySumm <- function(mod) {
   c(sigma_e = sigma(mod), sigma_r = sqrt(unlist(VarCorr(mod))))
}
booted_lamb_fitted <- bootMer(lamb_fitted, mySumm, nsim = 100, seed = 2047)
```

```{r}
library(boot) # for nice print-out
booted_lamb_fitted
```

```{r}
?simulate
?bootMer
```

Good reference of the R package `lme4`

- https://arxiv.org/pdf/1406.5823.pdf

- https://github.com/lme4/lme4

# Project 1

1. Reading: Write a few sentences to summary what is going on.
2. Data Entry: Input the data from the table in textbook so you can fit the model here in R. Find you way to do it.
3. Analysis using Maximum Likelihood: Fit the model with MLE, i.e. `REML = FALSE`.
   i. Asymptotic covariance matrix, a.k.a. textbook formula
   ii. Parametric bootstrap
4. Analysis using REML: Fit the model with MLE, i.e. `REML = TRUE`.
   i. Asymptotic covariance matrix, a.k.a. textbook formula
   ii. Parametric bootstrap
5. Discussion: Write something intelligent.

Don't hand in raw R output as your report. Write paragraphs and tables.

### Asymptotic covariance matrix

Study section 1.3 of the textbook carefully.

Long story short: Get the inverse of the Fisher information matrix of the $\hat{\theta}$'s. The ingredients you need are $X$ and $Z$. The recipe is section 1.3 of the textbook.



