---
title: "STA232Lab3"
author: "Po"
date: "1/20/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# HW 2.1

Consider the Neyman-Scott problem (Example 9).

(a) Show that the MLE of $\sigma^2$ based on
$y_{ij}$, $i = 1,...,m$,$j = 1,2$ converges in probability to $\sigma^2 /2$, and therefore is inconsistent.

(b) Show that the REML estimator of $\sigma^2$, that is, the MLE of $\sigma^2$ based on $z_i$, $i = 1, . . . , m$, converges in probability to $\sigma^2$, and therefore is consistent.

Read also Example 1.7 and Exercise 1.8 in textbook Jiang 2007.

Example 1.7 (Neymanâ€“Scott problem). Neyman and Scott (1948) gave the following example which shows that, when the number of parameters increases with the sample size, the MLE may not be consistent. Suppose that two observations are collected from m individuals. Each individual has its own (unknown) mean, say, $\mu_i$ for the $i$th individual. Suppose that the observations are independent and normally distributed with variance $\sigma^2$. The problem of interest is to estimate $\sigma^2$. The model may be expressed as the following, $y_{ij} = \mu_i + \epsilon_{ij}$, where $\epsilon_{ij}$ are independent and distributed as $N(0, \sigma^2)$. Note that this may be viewed as a special case of the linear mixed model (1.1), in which $Z = 0$. However, it can be shown that the MLE of $\sigma^2$ is inconsistent (Exercise 1.8).

#### Hints:

Use weak law of large numbers:

if $X_1, \dots, X_n$ i.i.d with $\text{E}(X) < \infty$ and $\text{Var}(X) < \infty$ then
$$
\frac{1}{n}\sum_{i = 1}^n X_i \xrightarrow{\text{P}} \text{E}(X)
$$

# HW 2.2

Exercise 1.12 of Jiang (2007).

1.12. Show that, under the balanced one-way random effects model (i.e., Example 1.1 with $k_i = k$, $1 \leq i \leq m$), the REML equations for estimating $\sigma^2$ and $\tau^2$ are equivalent to (1.22). Obtain the solution to these equations. Also derive the asymptotic covariance matrix of the REML estimators.

\begin{align*}
y_{ij} &= \mu + \alpha_i + \epsilon_{ij} \\
i &= 1, \dots, m \\
j &= 1, \dots, k \\
\alpha_i &\sim N(0, \sigma^2) \\
\epsilon_{ij} &\sim N(0, \tau^2)
\end{align*}

#### Hints:

Here I am trying to show you how the solution looks like, so you wouldn't waste too much time walking toward bad direction. You need to show by yourself how you get here.

$$
Y = ( y_{11}, \dots, y_{1k}, \dots, y_{m1}, \dots, y_{mk})'
$$

$$
X = 1_m \otimes 1_k
$$

$$
Z = I_m \otimes 1_k
$$

$$
V = I_m \otimes (\tau^2 I_k + \sigma^2 1_k 1_k')
$$

By Woodbury matrix identity,

$$
V^{-1} = I_m \otimes \frac{1}{\tau^2} \left( I_k - \frac{\sigma^2}{\tau^2 + k \sigma^2} 1_k 1_k' \right).
$$
Then, show off you skills in https://en.wikipedia.org/wiki/Kronecker_product

\newcommand{\by}{\mathbf{y}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\be}{\boldsymbol{\epsilon}}
\newcommand{\ba}{\boldsymbol{\alpha}}
\newcommand{\bb}{\boldsymbol{\beta}}

\newcommand{\bX}{\mathbf{X}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bOne}{\mathbf{1}}

\newcommand{\diag}{\text{diag}}
\newcommand{\Var}{\text{Var}}
\newcommand{\E}{\text{E}}

You should be able to get the $\bP$ like this

$\bP = \dfrac{1}{\tau^2}\diag(\bV_k,...,\bV_k) - \dfrac{1}{mk(\tau^2 + k\sigma^2)}\bOne_{mk} \bOne_{mk}'$, where $\bV_k$ is a $k\times k$ matrix with  diagonals $1 - \dfrac{\sigma^2}{\tau^2 + k\sigma^2}$ and off-diagonals $\dfrac{-\sigma^2}{\tau^2 + k\sigma^2}$.

Then try you best to simplify the matrix equations into the equations required by the exercise.

For the estimated asymptotic covariance matrix of $\hat{\theta}$, it will involve the matrices $\bZ$ and $\bP$. You can further express it in terms of things like $k$, $m$, MSA, MSE... but it is messy. If you can show it, it is the best. If you couldn't, show reasonable work!


# HW 2.3

Exercise 1.14 in Jiang (2007).

1.14. Show that the REML equations derived under the multivariate $t$-distribution (see Section 1.4.1) are equivalent to those derived under the multivariate normal distribution.

#### Hints:

I can guide you to show the REML equation under the ANOVA model ( equation (1.2) ).

Firstly, as a warm-up, try deriving REML equations (1.23) and (1.25) in Gaussian distribution in ANOVA model. It is exercise 1.13.

Now get to the t-distribution business. You should be able to get:
$$l_R(\theta) = c - \frac{1}{2}\log(|A'VA|) - \frac{n-p+d}{2}\log\left(1 + \frac{1}{d}y'Py \right) $$
and then the REML equation is
$$
(n-p+d)y'P\frac{\partial V}{\partial \theta}Py = tr\left( P\frac{\partial V}{\partial \theta}\right) (d+y'Py)
$$

Here, if $d+y'Py = n-p+d$, you get the REML equation exactly as with Gaussian.

Then, assume $\bV$ is in the form of ANOVA model, i.e. $V = \tau^2I_n + \sum_{i=1}^s\sigma_i^2Z_iZ_i'$. Plug-in the $\tau$ and $\sigma_i$ into the above. Then try to get $y'Py = n-p$.

Here are some equations you should be able to show:

$p = \text{rank}(X)$

$\text{rank}(A) = n - p$

$\text{tr}(PV) = n - p$.

$PVP = P$

# HW 2.4

For the Lambs data of Project 1, derive a Wald-test statistic, in as explicit form as possible, for testing the hypothesis that there is no difference among the lines. What is the asymptotic distribution of the Wald-test?

#### Hints:

Simply Page 177 in lecture note.

The model for the Lambs data of Project 1 is

$$
y_{ijk} = \mathbf{x}^\prime_{ijk} \boldsymbol{\beta} + s_{ij} + e_{ijk} = l_i + a_1 x_{ijk,1} + a_2 x_{ijk, 2} + s_{ij} + e_{ijk}
$$
$$
1 \leq k \leq n_{ij}
$$
$$
\boldsymbol{\beta} = (l_1, l_2, l_3, l_4, l_5, a_1, a_2)'
$$

The $i^{th}$ component of $\mathbf{x}_{ijk}$ is $1$ for $1 \leq i \leq 5$. The $j^{th}$ component, $1 \leq i \leq 5$ and $j \neq i$, of $\mathbf{x}_{ijk}$ is $0$. The $6^{th}$ component of $\mathbf{x}_{ijk}$ is $x_{ijk,1}$. The $7^{th}$ component of $\mathbf{x}_{ijk}$ is $x_{ijk,2}$.


You should be able to explicitly write down the matrices $\hat{V}$, $\hat{V}^{-1}$ in terms of $\tau$, $\sigma$ and $n_{ij}$. Then you explicitly write down $X' \hat{V}^{-1} X$ in terms of $\hat{\tau}$, $\hat{\sigma}$, $x_{ijk}$ and $n_{ij}$.

## Lamb data

Let's look at the Lamb dataset in the textbook (the table 1.2 in Jiang 2007, same as the table 1 in Harville 1985).

>    Harville, David A., and Alan P. Fenech. "Confidence Intervals for a Variance Ratio, or for Heritability, in an Unbalanced Mixed Linear Model." Biometrics 41, no. 1 (1985): 137-52. doi:10.2307/2530650. <br>
> https://www.jstor.org/stable/2530650

> For purposes of illustration, we introduce in Table 1 some data consisting of the weights at
> birth of 62 single-birth male lambs. These data (provided by G. E. Bradford, Department
> of Animal Science, University of California, Davis) come from five distinct population
> lines (two control lines and three selection lines). Each lamb was the progeny of one of 23
> rams, and each lamb had a different dam. Age of dam was recorded as belonging to one of
> three categories, numbered 1 (1-2 years), 2 (2-3 years), and 3 (over 3 years).

Sheep glossary: http://www.sheep101.info/201/glossary.html

`lamb_lab1.csv` contains a subset of the data (Line 1 and 2).

```{r message=FALSE}
library(lme4)

lamb <- read.csv(file = "lamb_lab1.csv")
cols <- c("Sire", "Line", "Age")
lamb[cols] <- lapply(lamb[cols], as.factor)

knitr::kable(lamb)
```

Let's fit a linear mixed effect model!
```{r}
lamb_fitted <- lmer(Weight ~ Line + Age - 1 + (1|Sire), data=lamb)
summary(lamb_fitted)
```


```{r}
lamb_fitted_mle <- lmer(Weight ~ Line + Age - 1 + (1|Sire), data=lamb, REML = FALSE)
summary(lamb_fitted_mle)
```


Lets do parametric bootstrap!
```{r}
mySumm <- function(mod) {
   c(sigma_e = sigma(mod), sigma_r = sqrt(unlist(VarCorr(mod))))
}
booted_lamb_fitted <- bootMer(lamb_fitted, mySumm, nsim = 100, seed = 2047)
```

```{r}
library(boot) # for nice print-out
booted_lamb_fitted
```

```{r}
?bootMer
```


# Project 1

1. Reading: Write a few sentences to summary what is going on.
2. Data Entry: Input the data from the table in textbook so you can fit the model here in R. Find you way to do it.
3. Analysis using Maximum Likelihood: Fit the model with MLE, i.e. `REML = FALSE`.
   i. Asymptotic covariance matrix, a.k.a. textbook formula
   ii. Parametric bootstrap
4. Analysis using REML: Fit the model with MLE, i.e. `REML = TRUE`.
   i. Asymptotic covariance matrix, a.k.a. textbook formula
   ii. Parametric bootstrap
5. Discussion: Write something intelligent.

Don't hand in raw R output as your report. Write paragraphs and tables.

### Asymptotic covariance matrix

Study section 1.3 of the textbook carefully.

Long story short: Get the inverse of the Fisher information matrix of the $\hat{\theta}$'s. The ingredients you need are $X$ and $Z$. The recipe is section 1.3 of the textbook.



